# 你可能感到疑惑的地方

## **1000 Tokens 是什么概念？（粗略估算）**

需要注意的是，这个数字仅仅是一个估算，具体的数字还可能会因为不同的上下文和语言模型而有所不同。

对于英文，官方也给出简洁的估算方式 [1](https://n3xtchen.github.io/NDigitalGarden/notes/2023-03-10-gpt3-token/#fn:1):

1 token ~= 4 chars in English

1 token ~= ¾ words

100 tokens ~= 75 words 或者

1-2 sentence ~= 30 tokens

1 paragraph ~= 100 tokens

1,500 words ~= 2048 tokens

简单总结，就是 **1000 Token 约等于750个英文单词**，67个句子，10个段落。

### **对于中文，如何计算 Token 量？**

在 **ChatGPT** 中，每个 **Token** 的长度并不是固定的，它们的长度是根据上下文和语言模型计算得出的。但是，对于一个通常长度的 **Token**，我们可以估算出大约需要多少个中文字符来达到相同的长度。

从官方文档可知，1000 Token 约等于 750 个英文单词，相当于 4000 个英文字符。根据一些研究的结果，英文单词的平均长度为 4 个字符，而中文单词的平均长度为 2 个汉字，那么**1000个token大约相当于2000个中文汉字**，200个句子（一般来说，一句话通常包含10个左右的汉字），另外一段话通常包含几个句子，其长度也会因文本类型、结构和主题而有所不同。

## **各个模型之间有什么区别？**

| **模型类型**  | **每1000个Tokens的费用($)** | **说明**                                                     |
| ------------- | --------------------------- | ------------------------------------------------------------ |
| gpt-4.0       | 0.06                        | 比 GPT-3.5 模型更强大，能够执行更复杂的任务，并针对聊天场景进行了优化。 会不断迭代更新 |
| gpt-3.5-turbo | 0.002                       | 目前主流的模型，便宜快速，推荐                               |
| Ada           | 0.0004                      | 更适用于文本分类、命名实体识别和语义理解等任务               |
| Davinci       | 0.020.0005                  | 最强（上一代）的 GPT3 的模型                                 |